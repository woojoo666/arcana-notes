
### Structure

```js
root = {
	repeaters: [ {template, container}, ... ],
	listeners: [ {template, elem, property}, ... ],
	value: {
		key1: {
			repeaters: [...],
			listeners: [...],
			value: "hello"
		},
		key2: {
			repeaters: [...],
			listeners: [...],
			value: {
				subkey1: {...},
			},
		},
		mArray: {
			repeaters: [...],
			listeners: [...],
			value: {
				0: {...},
				1: {...},
				2: {...},
			},
		},
	},
};
```

Wijits
-------

### handlebars
```html
<div>{{foo}}</div>
```
* events:
	* #updated
		* default listeners: render()
* triggers
	* set() -> #updated
* attach more listeners using `<div updated="myListener">{{foo}}</div>`, and `scope.myListener()` will be called when update is triggered
* if your listener contains the call `this.preventDefault()`, it will prevent the default listener from being triggered (in this case, `render()`)

### list wijit
```html
<repeat items="index, item in items">
	<div>{{index}}: {{item}}</div>
</repeat>
```
* events:
	* #item_changed
	* #index_changed
* triggers:
	* set() -> #item_changed
	* insert(), remove() -> #index_changed
* if an item is inserted to beginning of the list, then everything after gets its index changed
	* each event will cause the entire element to re-render!
	* thats a lot of rendering!! pretty expensive
* thus, check if there are any #index_changed listeners in the first place, before sending out all the events
* this should generalize to all events. For example, if there are no #item_changed listeners, no need to send out any events after set()

### selection wijit
```html
<selection items="item, isSelected in items" selection="selection" onselectionchanged="onselectchanged">
	<div class="{{isSelected}}">{{item}}</div>
</selection>
```
* events
	* #item_changed
	* #item_selected
	* #selection_changed
* triggers
	* select() -> #item_selected, #selection_changed
* note that isSelected is NOT a property of item
	* "item" is a data structure defined by the user, we don't wanna restrict them with reserved properties
	* instead, we have a mirror array specifically for properties like these

Scopes
------

* for things like the listwijit, that declare new variable names, I suspected that I would need to start dealing with creating new scopes and such
* originally, my simple solution was to replace the new variable names with their absolute path
* for example, if I had this (assuming `items` and `foo` are already declared in the root scope)

```html
<repeat items="item in items">
	<p>{{item}} hello {{foo}}</p>
</repeat>
```
* I could render it as something like this

```html
<container>
	<p>{{items.0}} hello {{foo}}</p>
	<p>{{items.1}} hello {{foo}}</p>
	<p>{{items.2}} hello {{foo}}</p>
	etc...
</container>
```
* problem with this is that if I inserted an item at say, index 0, then all the items get their absolute path changed
* another tempting solution is to walk up the tree to find the first match
	* {{item}} will walk up all the way till the repeat wijit, where it finds a match
	* {{foo}} will walk all the way up till the root scope
* problem is when you have something like this

```html
<repeat items="item in [5,10,15]">
	<p>{{item}} hello {{foo}}</p>
</repeat>
```
* now the scope of the repeat wijit is completely separate from the root scope
* thus, whenever we create a new scope we need to explicitly hold onto a reference to the parent scope
* analagous to this

```js
repeat(items, function (item) {
	return handlebars(item + foo);
})
```
* more on scopes further down

Data vs Function-Based
------------------------

* should I make it more like angular, where I directly modify data and the widgets "listen" to the data and change on their own
	* allows me to attach multiple different widgets to the same piece of data
or should I make it more function-based, where widgets have an API and an internal piece of data, and you call the widget's API to modify the data
	* more object oriented, like `selectList[4].select()` instead of `selectList[4].isSelected = true;`
	* this makes sense with the "repeat" wijit, where you would have common array operations like "push" and "splice"
	* however, this means that I can only have one type of widget attached to a piece of data, otherwise the different APIs might clash
* example: music player

### Angular
```html
<repeat for="song in songs">
	<img src="song.cover"></img>
	<button onclick="play(song)">{{song == currentlyPlaying && song.playing ? pause : play}}</button>
	<p>{{song.title}}</p>
</repeat>
<script>
var songs = retrieveSongs();
var currentlyPlaying;

function play (song) {
	if (currentlyPlaying == song) {
		if (song.playing) {
			audioPlayer.pause();
			song.playing = false;
		} else {
			audioPlayer.resume();
			song.playing = true;
		}
	} else {
		audioPlayer.play(song.src);
		currentlyPlaying = song;
		song.playing = true;
	}
}
</script>
```

### Function Based
```html
<template name="song">
	<img src="cover"></img>
	<button onclick="events.play">{{playing ? pause : play}}</button>
	<p>{{title}}</p>
	<script>
	setPlaying = function (bool) {
		this.playing = bool;
	}
	</script>
</template>
<songlist for="song in songs">
	<song data="song" onplay="play(song)"></song>
</songlist>
<script>
scope.songs.setAll(retrieveSongs());

scope.play = function (song) {
	if (currentlyPlaying == song) {
		if (song.playing) {
			audioPlayer.pause();
			song.setPlaying(false);
		} else {
			audioPlayer.resume();
			song.setPlaying(true);
		}
	} else {
		if (currentlyPlaying) {
			currentlyPlaying.setPlaying(false);
		}
		audioPlayer.play(song.src);
		currentlyPlaying = song;
		song.setPlaying(true);
	}
}
</script>
```

Wijit Model
-------------

```html
<wijit name="song">
	<img src="{{cover}}"></img>
	<button onclick="{{onplay}}">{{active && playing ? pause : play}}</button>
	<p>{{title}}</p>
</wijit>
<repeat for="song in songs">
	<song data="song" active="song.src == audioPlayer.src" playing="audioPlayer.playing" onplay="(e) => play(song, e)"></song>
</repeat>
<script>
audioPlayer = {
	states: { loading, loaded, finished, error },
	playing: false,
	currentTime,
	src,

	play: function (src) {
		this.src = src;
		state = loading;
	},

	restart: function () {
		this.currentTime = 0;
		state = loaded;
		playing = true;
	},
}

songs = retrieveSongs();

function play (song, e) {
	if (song.active && song.state !== error) {
		if (song.state == finished) audioPlayer.restart();
		else audioPlayer.playing = !audioPlayer.playing;
	} else {
		audioPlayer.play(song.src);
	}
}
</script>
```

* originally I was writing array functions into the repeat wijit, like push() and insert()
* this way I could trigger the correct element listeners after modifying the underlying array
* also considered having <repeat> wijit like a library api, where you would pass in the array to modify, eg instead of push(item), do push(array, item)
	* this way the array wasn't "internal" to the <repeat> wijit, and multiple wijits could bind to one array
	* if other types of wijits wanted to use the push function, they could borrow the function: RepeatWijit.push.call(this, array, item)

* in the end, its not whether or not it should be data based or function based, everything has data and functions
* but for wijits, data is in the form of states, and functions are in the form of state transitions
* for the underlying data structures, data is just data, while functions are used for transforming the data
* the <repeat> wijit shouldn't have the push() or splice() functions because that is something inherent to the Array data structure
	* something only the Array data structure should have control over
* <repeat> wijit can attach listeners to the push() or splice() events to respond accordingly
* the <repeat> wijit can still respond to user events and modify the underlying Array, but it has to do so through the Array's transformation api

### Wijit Binding

* notice the `active="song.src == audioPlayer.src"` wiring, kind of like how in verilog you can do

```
SongModule song0 (
	.data (song),
	.active (song.src == audioPlayer.src),
	.playing (audioPlayer.playing),
	.onplay (play),
);
```

* `onclick="{{onplay}}"` declaring an outgoing event, and `onplay="(e) => play(song, e)"` is where it is bound
	* yeah I know arrow functions are for ES6, but it makes it way more concise
	* there's really nothing special going on here, "onplay" is just a property of song, so onclick calls song.onplay(e)
* note that handlebars are only needed with default html tags, and are implied in wijits

### Bind Expressions

```js
scope.bind([scope.var1, scope.var2], function () {
	scope.x = scope.var1 + scope.var2;
})
```
* this example will bind the result of `scope.var1 + scope.var2` to `scope.x`
* syntax for bind function: `bind(result_variable, [trigger_variables], evaluator)`
* all wijits internally use these bind expressions
* `<div>{{foo}}</div>` is just a special bind expression that, instead of setting a variable, sets the innerHTML of an element

shorthand:

```js
scope.bind('x = var1+var2');
```
* right hand side must be simple expression (single statement of words and operators, no "," or ";" or "function")
* also no function calls, because we can't determine which variables to watch from that (use long-form bind to manually specify trigger variables)
* take out left-hand-side -> "var1+var2"
* find all variables that consist of just letters and "."s: `[\w.]*`

dynamic trigger variables:

* problem: "x = a[b+c]", we shouldn't be watching if `a` changes, we have to watch if `a[b+c]` changes
* `a[b+c]` defines the trigger variable
* when it changes, we need to rebind this expression
	* remove listener from old trigger variable
	* attach to new trigger variable
* this is a bit complicated, so I won't allow `[]` characters in bind expressions for now

### Scopes and Binding

* so we don't have to explicitly hold onto each scope and variable name, we let javascript closures do the work for us

```js
function parse (scope, elem) {
	triggerVars = [];
	expression = elem.innerHTML.replace(/[\w.]+/g, function (match) {
		if (!match[0] || !isNaN(match[0])) return match; // skip names that start with a number (invalid)
		triggerVars.push(match); // add name to triggers
		return 'scope.' + match; // prepend "scope." to all variable names
	});
	var evaluator = eval('(function () { elem.innerHTML = ' + expression + ';})'); // enclose expression in a function, capturing the scope at the same time
	scope.bind(triggerVars, evaluator);
}
```

* when you need to create new scopes

```js
function parseRepeater (scope, container, template) {
	var repeatExpr = container.repeat.split(' in '), // given expression "item in items"
		data = scope[repeatExpr[0]],
		alias = repeatExpr[1]; // data: scope["items"], alias: "item"

	data.forEach(function (item) {
		var extendedScope = Object.create(scope); // create a new scope off the parent scope
		extendedScope[alias] = item; // add the extra "item" variable
		var elem = template.clone();
		parse(extendedScope, elem); // parse will create the listeners with the new scope, and bind listeners to the new element
		container.append(elem);
	});
}
```

* the function enclosure inside the `eval()` is the key to everything
	* pre-compiles the expression, which has [up to 18x performance benefits](http://jsperf.com/eval-vs-pre-compiled/4)
	* evaluates the expression for us so I don't have to make my own expression parser
	* holds onto the scope so I don't have to manually store a reference to it
	* leverages javascript's built in closure and scope mechanisms, ensuring consistency between my framework and doing everything in vanilla js

### Binders

* originally I was gonna try to leverage the `bind("x = a+b")` function for binding elements
* issue: different method signatures:
	* `bind (scope, 'x = a+b') => scope.x = scope.a + scope.b`
	* `bindElem (scope, '{{a+b}}', elem) => elem.innerHTML = scope.a + scope.b`
	* the left-hand side of bind() is within the scope, while bindElem() is not
	* maybe there's a way to coax one of the functions to look like other?
* realized that bind() and bindElem() do fundamentally different things
* bind() is meant for binding data together within the same scope
* bindElem() is meant for binding data between the app scope and the element scope
* thus, while the syntax is similar now, the syntax can diverge at any time, so bind() and bindElem() should be kept separate
* wijits and manual bind() expressions all are types of **binders**
* binders are responsible for maintaining data bindings
* internal binders like bind() expressions are for binding data within the same scope
* binders like bindElem() are for binding data across scopes
	* eg: a database binder that updates the database when the user changes data in the app
* wijits create new scopes and bind to the parent scope
* ultimately the wijit tree leads to elemwijits, special wijits that use bindElem() to bind directly to a DOM element

### Tracking Array Index

* say you have a repeater that keeps track of index
	`<repeat for="item,index in array">`
* should the index be handled by the array data structure, or the repeat wijit?
	* data structure: wrap item and index in a container `{ item: ..., index: ...}`
		* array stores these containers, not the items directly
		* when index of a container changes, container.index property updated to match
	* repeat: keep an array of all the extendedScopes, store an `index` property in each of these extended scopes
		* update affected extendedScopes.index properties when recieving #oninsert, #onremove events
* the problem with handling it in the array structure is that we have this indirect wrapper structure going on
* the problem with handling it in repeater, is if we have another wijit that wants to use indexes, we have to reimplement all the index updating functions when receiving #oninsert and #onremove
* SOLUTION:
* note that index is not a part of the item
* array uses it internally to get/set items, but the item itself is detached
* for this specific repeater, we want each item to contain both the item and the index
* use a special array that wraps the item and index
* thus, its not handled by the array structure or the repeat wijit, but a new data structure entirely

### Deep Property Binding

* `x = a.a.a`
* this can be seen as `x = a.get('a').get('a')`
* three ways to handle this
	* keep as one binding, with three triggers `[a, a.a, a.a.a]`
	* separate into two bindings `temp = a.get('a')`, `x = temp.get('a')` with two triggers each
	* one binding, one trigger `a.a.a`, but when triggering listeners, search the tree to trigger all child listeners too
		* eg if we have `x = {a: {a: 3}, b: 2 }`, and we set `x`, then trigger listeners for `x`, `x.a`, `x.a.a`, `x.b`

### Collection Listeners

* ignoring efficiency, whats the simplest collection listener?
	* if we start with `map = {a: 1, b: 2}`, and a collection listener `for key,val in map => map2[key] = val+1`
	* initially the collection listener creates 2 value listeners, `map2.a = map.a+1`, `map2.b = map.b+1`
	* set `map.b = 3` => value listener changes `map2.b`
	* set `map.c = 4` => collection listener notices keyset changes, destroys old `map2` and all value listeners, then re-evaluates entire collection to create 3 value listeners
* what about arrays?
* if we do `['a','b','c'].shift('x')`:
	* item 0 changes to 'x', item 1 changes to 'a', item 2 changes to 'b', item 3 changes (from undefined) to 'c', and length changes to '4'

### Separating Definitions from Data, Bindings from Values

* binding should happen at the definition level, not the data/value level
	* if we bound listeners to data/values:
		* say we bind `x = a.a.a` => `a = {a: {a: 5, _listeners: ["x = a.a.a"] }}`
		* then if we set `bla = a.a.a`, `bla` would inherit the listeners
		* in addition, if we set `a.a.a = 10`, then we would lose the listeners
* I already separated the listeners from the value at the tip level
	* `x = {listeners[], value}`, if you change x value, don't have to rebind the listener
* however, for `x = a.a.a`, if you set `a = {a: {a: 5}}`, have to rebind listeners for `a.a` and `a.a.a`
* thus, we need to completely separate the bindings tree from the value tree
* when setting a value:
	* set the value in the value tree
	* follow the same path in the bindings tree to retrieve the evaluators and run them
		* make sure to trigger any collection listeners directly above the path, eg `a.a.a = 5` will trigger collection listeners on `a.a`
	* this process is recursive; evaluators will set a value, which can trigger more evaluators

```js
a = {a: {a: 4}};
b = 11;
x = a.a.a; //binding 1
y = x+b; //binding 2
z = a.a*b; // binding 3
```
will result in:

```js
// eval1: (x = a.a.a), eval2: (y = x+b), eval3: (z = a.a*b)
bindings: { "a": [eval1], "a.a": [eval1, eval3], "a.a.a": [eval1], "b": [eval2, eval3], "x": [eval2] }
values: { a: {a: {a: 4} }, b: 11, x: 4, y: 15, z: 44}
```

### Event Listeners for Optimization, Re-evaluation by default

* bindings work purely on the current state of the data, regardless of the operations used to get to that state
* when the data changes, we simply re-evaluate the expression
* this is why my previous idea in "Tracking Array Index" with registering #oninsert and #onremove listeners is wrong
	* also my current implementation of ReactiveArray sends #onpush events to update listeners, which is also wrong
* listeners should not have to implement an interface or listen to events. This part is data based, not function based
* note that events can be used to optimize the re-evaluation

ReactiveArray.splice(4,2,["a"]) => onsplice
	ReactiveArray.remove(4,2) => onremove
		ReactiveArray.removeOne(4) => onremoveOne
		ReactiveArray.removeOne(4) => onremoveOne
	ReactiveArray.insert(4,["a"]) => oninsert
		ReactiveArray.insertOne(4,"a") => oninsertOne

* lets say this ReactiveArray has 3 listeners:
	* listener 1: implements onsplice
		* absorbs the #onsplice event, and is not called for subsequent events
	* listener 2: implements onremoveOne and oninsert
		* absorbs two #onremoveOne events and one #oninsert event
	* listener 3: implements onremove
		* can absorb the onremove tree but not the oninsert tree, default to re-evaluation

* a binding should always be able to default back to re-evaluating the expression
* note that setting a just one property can trigger a complete re-evaluation
* originally, in my repeat wijit, modifying one property just affected the corresponding item element
* however, that won't work for something like a summation function: `array => array.reduce((sum, item) => sum+item, 0)`
	* no #onset listeners so defaults to re-evaluation
* on the other hand, operations like map() can automatically generate #onset listeners

```js
bind('squared = array.map(x => x*x)');
// results in
var mapFn = (x => x*x)
evaluator = () => { scope.squared = scope.array.map(mapFn) }
evaluator.onset = (key, value) => { scope.squared[key] = mapFn(value) }
```

* map binds can also use item-to-item bindings in place of onset (which is how the repeat wijit works)

```js
bind('squared = array.map(x => x*x)');
// results in
var mapFn = (x => x*x)
evaluator = () => { scope.array.forEach(evaluator.onpush) } // use onpush to create item-to-item bindings between scope.array and scope.squared
evaluator.onset = () => true // don't do anything
evaluator.onpush = (value, index) => { scope.squared.push(value); scope.bind('squared['+index+'] = mapFn(scope.array['+index+']') }
```

### ReactiveArrays

* note that if we use an internal array and use array operations, we have to make sure to manually call listeners
* eg, ReactiveArray([1,2,3,4,5]).splice(2, 0, "x") has to both update any array/collection listeners, but also call value listeners for each array item after (and including) index 2

### Proxies and getters/setters

* use `defineProperty` to override get()/set() so you can use normal assignment notation while still triggering the listeners

```js
function createProxy(obj, prop) {
	var value = obj[prop];

	Object.defineProperty(obj, prop, {
		get: function () {
			return value;
		},
		set: function (newValue) {
			value = newValue;
			// call all listeners
			obj._listeners[prop].forEach(function (listener) { listener(); });
		},
		enumerable: true,
		configurable: true
	});
}

var test = {a: 10}; // not proxied yet
createProxy(test, 'a');
test.a = 5; // proxied, calls listeners
```

* during the bind() function, we can extract the trigger variable names and proxy them
* however, note that because we have to proxy one by one, we can't dynamically proxy every item in a collection
* thus, collections listeners won't be able to detect when new properties are added
* good try pupper

### Pre-parsing

* because proxies won't work, perhaps we can do pre-parsing
* all wijit script tags are within a `<template>`, so they won't automatically be run
* before running a wijit script, find all assignments and replace them with set()
* eg `scope.a = "hi"` => `scope._set('a', "hi")`
* make sure that set() returns the result to account for expressions like `var x = (scope.x = 5)` => `var x = (scope._set('x', 5))`
* a possible issue with this is finding where to close the parenthesis
* perhaps we can replace the assignments with proxies
* eg `scope.x = "hi"` => `scope._proxy('x').$ = "hi"`

```js
scope._proxy = function (path) {
	return {
		set $ (val) {
			return scope._set(path, val);
		}
	};
};
```

### Deep Property Binding Revisited

* consider a simple assignment, `foomirror = foo`
* due to javascript's reference-based model is that, if foo.x changes, bar.x will change too (foo and bar point to the same object)
* if foo itself changes though, bar won't, because now foo and bar point to different things
* our binding mechanism fixes this, and mirrors changes at the base level:
* however, while the reference-based model seems like a blessing, mirroring changes above the base level, this is actually a problem because it _silently_ mirrors changes
* for example, if we bind `x = foomirror.x`, and we change `foo.x`, `foomirror.x` changes without the binding mechanism knowing about it, and `x` won't be updated
* to fix this, when changing a value, don't just trigger the listeners on that value, trigger the listeners on any ancestors of that value as well
	* we also trigger the listeners on any descendents too, as noted in the "Deep Property Binding" section
* if "x" marks the set value, "a" marks every affected node

	       a
	      / \
	     a   .
	    / \   \
	   .   x   .
	      / \
	     a   a

* this is the full **#onset event propagation**
* tbh, this makes perfect sense. if foo.x changes, that means foo changed as well
* but what about collection listeners?

* consider if we bind `bar.x = foo`, and the later bind `bar = foo`
* this gives `bar.x` a double binding, because the binding `bar = foo` will also determine `bar.x`, BAD!!
* to fix all these issues, we force all binding results to be at the root level
* this also prevents multiple ways of defining identical bindings, which could happen before:
	* `bind(x = {a: foo, b: bar})` == `x = {}; bind(x.a = foo); bind(x.b = bar)`
* this also prevents mixed items, where some properties are bound and some aren't, eg `x = {a: 0}; bind(x.b = foo)`
* in fact, all bindings should belong in a separate domain entirely

```
module songelem (song, audioPlayer) {
	active: (song.src == audioPlayer.src)
	playing: audioPlayer.playing
} (active, playing)
```

* this also ensures that all bindings happen at once, at the beginning
	* if a binding didn't happen at the beginning, then the period of time which it was unbound will likely lead to unexpected behavior
* note that bound variables can't really be used in functional code, but functional code can be used to change bound variables

### Collection Listeners Revisited

* don't item-to-item bindings also create double bindings?
* they also won't be at the root level
* collection bindings actually are for defining collections of bindings, not just for binding collections
	* thus, the "reduce" function discussed in the "Collection Listeners" section is not actually a collection binding!
* collection bindings determine how and when to create/destroy bindings, and create a structure to store the values
* the bindings themselves determine the values
* that way there's no conflict of interest
* the indings they create "belong" to the collection, called **sub-bindings**
	* during #onset event propagation, the sub-bindings absorb the event for the collection listener
	* if no sub-bindings absorb the event, then it reaches the base collection listener, and the whole collection binding is re-evaluated
		* all old sub-bindings thrown away, new ones created
	* this is why the sub-bindings don't have to be at root level 
	* all sub-bindings have to be between children of the trigger collections and children of the output collection
		* ensures that the sub-bindings will absorb the #onset
* collection bindings are bindings themselves, so they can create recursive structures, like a 2d matrix bind

```js
function songlist (songs, audioPlayer) {
	songelem: function (song) {
		active: (song.src == audioPlayer.src)
		playing: audioPlayer.playing
	}
	elems: songs.map(songelem) // collection bind. if a single song changes, then the corresponding _songelem changes, nothing else
}
```

* a normal mirror bind, `bar = foo`, is also a collection bind!
	* in a value-based model, this would be a vital optimization
		* if `foo.x.x.x` changes, then we should only update `bar.x.x.x` and all it's subvalues
	* in a reference-based model, the optimization is unneeded

### Reference Based Models

* its important to note that binding makes the most sense in a value-based model
* a reference based model is useful for passing around data structures, which doesn't really happen in data binding
* reference based models are also useful for copying data structures
* in some ways, reference based model may seem like they have inherent data-binding

```js
var foo = [5];
var bar = foo;
foo[0] = 10;
console.log(bar[0]); // outputs 10
```

* however, this only works because we are copying values
* if we added any transformations, this would not work, and the distinction between reference-based models and data-binding becomes clear

```js
var foo = [5];
var bar = foo.map(x => x*x)
foo[0] = 10;
console.log(bar[0]); // outputs 25, not 100
```

* thus, pass-by-reference is merely an optimization, and when thinking about data-binding, it's useful to assume a pass-by-value model

### Defining Collection Binds and Functional Programming

* functional programming is optimal for defining collection binds because functional programming never modifies or moves around data
* likewise, it doesn't make sense to modify or move a binding (note: dynamic bindings do move around, but that's a weird exception)

```js
function concat (list1, list2) {
	var res = new Array(list1.length + list2.length)
	for (var i = 0; i < res.length; i++) {
		res[i] = (i < list1.length) ? list1[i] : list2[i-list1.length] // bind the beginning to list1, and the rest to list2
	}
	return res
}
function flatten (tree) {
	if (!tree.length) return tree; // either an empty list or a value
	var last = tree.last();
	return concat( flatten(tree.slice(0, tree.length-1)), flatten(last) )
}
```

### Deep Property Binding and Collection Listeners Revisited (Again)

* collection binders are responsible for creating/destroying bindings
* the "value" of a collection is its pointers, not the values pointed to by those pointers
* thus, only monitor the pointers (collection keys), and the bindings created for those pointers will be responsible for monitoring the values at each pointer
* that way no need to monitor "sub-bindings"
* collection bindings are just like normal bindings, just triggered in a different way
	* regular bindings get triggered when value changes
	* collection bindings are triggered when key added/removed

```js
scope.set = function (collectionPath, key, value) {
	collection = get(collectionPath);
	var prev = collection[key]
	collection[key] = value
	if (prev == undefined) trigger(collectionPath) // key added, trigger collection binding
	trigger(collectionPath + '.' + key) // trigger regular binding
}
scope.delete = function (collectionPath, key) {
	collection = get(collectionPath);
	delete collection[key]
	trigger(collectionPath + '.' + key) // trigger regular binding
	trigger(collectionPath) // trigger collection binding
}
```

* also events are no longer propagated to ancestors (b/c children are now responsible for monitoring their own values)
* this means the normal assignment operator won't work for collections anymore
	* if we do `foo = bar`, and then `bar.x = 5`, binding for `bar.x` will be triggered but not the binding for `bar`, so `foo` won't get updated
* overload the assignment operator `=` to do a recursive mirror bind when acting on collections
	* if `foo` and `bar` are collections, `bind(foo = bar)` => `mirrorbind(foo, bar)`

```js
function mirror(source, dest) {
	if (source instanceof Object) {
		for (var key in source) {
			mirror(source[key], dest[key]);
		}
	} else {
		bind(source, dest);
	}
}
```

* static collection assignments like `foo = { x: bar.x, y: bar.y }` are no longer collection bindings anymore, because `foo` is not monitoring anything

### Reduce Bindings

* so far our design has been based on map bindings, but what expressions that work on an entire collection, not each child individually
* to best understand this, look at the difference between a map binding and a reduce binding

```
map binding:

    / a - bar.a
foo - b - bar.b
    \ c - bar.c

reduce binding:

    / a \
foo - b - sum
    \ c /
```

* we need to implement the dynamic many-to-one binding seen in the "reduce binding"
* we leverage our collection listener again, except unlike in map-bind where we create a binding for each child, we create a trigger for each child, and all the triggers lead to one reduce expression
* we can extend this to "deep" reduce functions, that act on trees and not just arrays
* eg a `sumAll` function that sums all numbers in a collection
* just push all numbers into an auxiliary array that is used for the reduce function

```js
temp = []
function sumAll (obj) {
	if (obj.length) obj.mapbind(sumAll)
	else temp.push(obj)
}
sum = temp.collectionListener(function () {
	var res = 0
	for ( var i = 0; i < temp.length; i++)
		res += temp[i]
	return res
});

foo.mapbind(sumAll)
```

* note that we could have leveraged a shallow reduce function and used recursion to get the same functionality
* this way we wouldn't need an auxiliary array for the reduce binding, just listen to each layer in the collection directly

```
deep binding:

        / m \
    / a - n \|
foo - b --- sum
    \ c ---/

recursive shallow binding:

        / m \
    / a - n - temp \
foo - b ------------ sum
    \ c -----------/
```

### Non-Root Level Bindings

* assignments like `foo = { x: bar.x, y: bar.y }` are not collection bindings, and contain non-root level bindings
* to prevent double bindings, we force non-root level bindings (eg `foo.x`) to be created at the same time as the container (eg `foo`)
* thus, we are not allowed to do things like `foo = {}; foo.x = bar.x`
* if we want to have "changing" bindings, we can always do something like `foo = loaded ? bar : "loading..."`

### Dangling Deep Property Bindings and Entry Nodes

* lets say `foo = { x: bar.x, y: bar.y }` and `fizz = foo.x*foo.y`
* what happens to foo when we set `bar = 3`?
* what happens to fizz if we set `bar = {x: 5}`?

* two main possibilities to handle hanging bindings, defaulting to `undefined` or throwing an error
* default value of undefined
	* makes it easy to handle 
	* more flexible, objects like `foo` can change structure without throwing an error
	* silent failures :(
* throw error
	* matches javascript way (`x = {}; y = x.x.x` => throws error)
	* strict typing mindset. In above example, `foo` follows a `{ x: ..., y: ...}` structure, and should always follow that structure, otherwise throw error

* lets change `bar` into a bound variable,`bar = buzz`, with `buzz` starting with a value of `buzz = {x: 3, y: 3}`
* this is a mirror bind
* if we change `buzz = {x: 4, y: 5}`, then the mirror bind destroys bar's old child values and bindings, and creates new ones
* for `bar`, `bar.x` and `bar.y` are temporarily undefined
* because this temporary undefinedness is inevitable, we can't throw an error otherwise we would have errors all the time
* this extends to any level, e.g. if we do `a = b.x.x.x` and `b = null`, `a` should be undefined
	* this is different from javascript, where an error would be thrown

* note that an optimization would be to keep the `foo.x` and `foo.y` bindings and instead just replace the value (instead of having temporary undefinedness)
* in a strict typing mindset, perhaps this would not be an optimization, since the structure of the value that we set `foo` to should never change
* thus, in a strict typing mindset, perhaps throwing an error would be better

* in addition, note how event propagation is no longer needed to the children
	* when bar's new `x` and `y` children are created and bound to `buzz`, their values will be updated, which will in turn update `foo`
* mirror binds replace event propagation
* also slightly more efficient than event propagation
	* only updates the subtree that need updating, instead of moving up the tree and updating the entire tree
* **entry nodes**: nodes whose value we set directly
	* in above example, `buzz` is the entry node
* as shown above, for everything to work properly, entry nodes need event propagation
	* setting `buzz = { x: 4, y: 5}` should trigger all bindings to `buzz`, `buzz.x`, and `buzz.y`
* instead, we can insert a mirror binding after each entry node
* to do this, we create a separate tree for entry nodes, and then mirror bind a node in the main tree to the node in the entry tree

```js
FloModule.set = function (key, value) {
	this.get('inputs')[key] = value;
	this.mirrorbind(key, '_inputs.'+key);
}
```

### FilterBind, MapBind, ReduceBind

as implemented in FloModule.js:

```js
function mapbind (dest, src, fn) {
	this.bind('devnull', [src], ar => {
		this.setVal(dest,[]);
		ar.forEach((_, i) => this.bind(dest+'.'+i, [src+'.'+i], fn));
	});
}

function reducebind (dest, src, fn, start) {
	this.bind('devnull', [src], ar => {
		var triggers = ar.map((_,i) => src+'.'+i);
		test.bind(dest, triggers, () => ar.reduce(fn, start));
	});
}

function filterbind (dest, src, fn) {
	this.reducebind(dest, src, (acc, x) => fn(x) ? acc.concat(x) : acc, []);
}
```
* notice how each binding contains two levels: an outer bind and an inner bind
* this makes sense, because each binding monitors two things: the array pointers, and the values pointed to

### Mirrorbind Everywhere

* remember that we are using mirror binds instead of event propagation
* earlier we said that "input nodes" need propagation
* we can get around this by giving each input node a preliminary mirror bind
* more specifically, input n
	* `mFloModule.set('foo')`

```js
function concat (dest, src1, src2) {
	this.bind('devnull', [src1, src2], (ar1, ar2) => {
		var temp = [];
		temp.length = src1.length+src2.length
		this.set(dest, temp)
		for (var i = 0; i < temp.length; i++) {
			this.mirrorbind(dest+'.'+i, i < src1.length ? src1+'.'+i : src2+'.'+(i-src1.length))
		}
	});
}
```

### Circular References

* something I forgot to account for is circular references, eg `foo = { a: { x: 5 }}`, `foo.a.y = foo`
* this would not exist in a value based model
* a mirror bind will run infinitely
* remember, we are using a value-based model because we are monitoring values, not references
	* if we monitored references, then `foo = bar` would not be triggered when `bar.x` changes
* perhaps we can copy by reference, but monitor by value
* in order to prevent infinite loops, during the mirror bind we tag the nodes that we have already bound, so that we don't run indefinitely

### Aliases

* constantly monitoring and copying mirror bindings is slow and expensive, especially now that we have to do this extra tagging and checking
* mirror bindings are extremely common, eg filter function on an array of objects will have many mirror binds
* copying by reference is fast
* perhaps there's an alternative way for an alias to mirror bind and "inherit" all triggers
	* inherit: if `foo = bar`, and `a = foo.x`, `foo.x` should inherit all triggers from `bar.x`
* one idea is to trigger based on *memory location*, not by variable names
* if `foo = bar`, and `bar.x` is changed to 3, then `foo.x` is also triggered because it points to the same memory location as `bar.x`
* but to make bindings based on memory location, we have to ensure memory locations for every variable stay the same
* eg if `foo = JSON.parse(msg)`, then `foo` can be any object
	* `bar = foo.x`, if `foo.x` disappears and reappears, it must be assigned to the same memory location to ensure that `bar` is updated
* this could potentially be implemented in the hardware side

* however, for now, an alternative would be a cross between propagation and mirror bind: lazy mirror binding
* for `foo = bar`, initially, only the initial reference is copied, and a binding at the root
* when `bar.x` is changed, the event propagates upwards until it reaches the root binding, where it realizes it is part of a mirror binding, and updates foo accordingly
* in addition to updated foo accordingly, it creates a binding for every node that it touched during propagation
* this way, if any node is changed again, it will just trigger the binding instead of propagating
* if no binding is found during propagation, a flag is set on every one of the touched nodes to indicate that propagation is no longer needed
* remember that this all works because *bindings are all defined in the beginning*
	* so if no bindings were found during initial propagation, then there will never be a binding on those nodes

* hmmm...what if we redirected all alias bindings to the source
* eg if `foo = bar`, and `x = foo.x`, then internally `x` will actually be bound to `bar.x`
* this shouldn't cause any problems because bindings don't change, so the source never should change either
* this can be thought of as binding to data/memory instead of binding to variables
* we bind to the source of data/information
* eg, if `foo = JSON.parse(msg)`, then `foo` is a new source of information
	* if we then do `fooalias = foo`, `fooalias` is not a source, so the binding goes to `foo` (the real source) instead of `fooalias`
* sources are mainly entry nodes or the results of calculations/parsing
* but why does this work?

### Reductions

* this is an example of a reduction
* `x = foo.x`, `foo = bar`, so thus we can infer that `x = bar.x` (and make that binding directly)
* another example of a reduction would be `a = b+c`, `d = a+b`, so we can infer that `d = 2b+c` and skip `a` (if its not used elsewhere)
* reductions are all optimizations
* in addition, the more complex the reduction, the less value it has
* for now, the only reduction we'll use is the alias binding reduction mentioned earlier
* this is because its simple, but massively reduces computations (no more graph-traversal mirror binds)



foo = bar
x2 = foo.x**2

note that we can change foo.x in two ways:
	* `bar = {x: 5}`
	* `bar.x = 5`
however, usually in flo, being able to change a variable in two ways causes conflicts, eg `a = b, a = c`
* however, both these ways must update foo.x using the same mechanism!
* in Flo, we can never have two functions updating the same value, to prevent conflicts
* thus, we have to choose one: either `foo` updates `foo.x`, or `bar.x` updates `foo.x`
* obviously makes more sents to have `bar.x` update `foo.x`
* this

bar = {x: 5}
bar.x = 3

in a way, the `.` (reference) operator acts like any other function
however, unlike functions, they can both be set manually and automatically
they are the only thing that is able to do something like that

to prevent this double pointers, we choose one of them
makes the most sense to choose the `bar.x -> foo.x` instead of `foo -> foo.x`


so far I've been treating `foo.x` as data, just like `foo` and `bar`
however, `foo.x` acts like a function, something like `foo.get('x')`
just like any other function, the `.` operator has an internal implementation
	while it might be obvious to just store the "address" of the value, and then use `.` to dereference the address, we can think of variations
	eg, store 2*address, and then divide by 2 when dereferencing
thus, when you do `bar = foo.x`, its like saying `bar = foo.get('x')`
when you do `bar = foo.x.x`, its like saying `bar.get('x').get('x')`, so this seems like its actually creating a binding to foo!
when we do something like `foo.x = 5`, that's where things get weird, because its not like we can do `foo.get('x') = 5`

when `foo.x` changes, its really saying "change the value at the memory location pointed to by foo.x"
so if we had another binding, like `foo.x = sourceX`, then this would be like changing `sourceX`
when `sourceX` changes, we somehow need to recognize that `foo.get('x')` has changed as well

usually in functions, we can determine the triggers through the variable names
example: `xy = x*y` -> x and y are the triggers
another example: `xy = fn(), fn = () => x*y`, we inspect the body of `fn` to determine that once again, x and y are the triggers
however, in `foo.get('x')`, there are no variable names besides foo (which we already know is a trigger)
but what is inside the body of `get`? in this case, it should return the value at an anonymous memory location
that anonymous memory location is the trigger
thus, triggers are defined through memory locations, not variables!
in our memory based system, `foo.x` and `bar` point to the same memory location, so changing either triggers both
a memory based system makes sense when working with pointers and references
for a variable based system, `bar` would need to be bound to `foo.x`, which I guess might work, just not as efficiently
	a variable based system makes it look more like a value oriented language
	this is because in a value based system, each variable gets its own memory location (no two variables pointing to the same memory)
	however, remember that circular pointers don't work in a value-oriented language :/

using the `.` operator on the left hand side of an assignment is syntactic sugar!
it merely allows us to modify memory locations without giving an explicit name
however, if you think about it, this is not usually allowed
eg, we can't do `foo.getX() = 5` to set the first element, even though we can do `foo.x = 5`
theoretically, `foo.getX() = 5` could be implemented:
	foo.getX() returns a value at a memory location, change the value at that memory location to 5
	if the function happens to return a dynamically-created temporary value, like `foo.toString()`, then simply erase that memory location when you leave the scope
NAHHH this is not true, because usually when u do `foo = bar.x`, foo is a new memory location



usually for a function, like `foo.toString()`, we don't know if the output is a dynamically-created temporary value, so assigning to it might have no effect
	if its dynamically created every time, then after assigning to the memory at `foo.toString()`, there will be no way to access the new value
	because a second call to `foo.toString()` will just create a new temporary value
however, for `foo.x` or `foo.get('x')`, we know for sure that it points to a memory location, so we can always assign to it



`foo.x = 5` is syntactic sugar for `foo.set('x', 5)`
`foo.a.x = 5` is syntactic sugar for `foo.get('a').set('x')`
note that these setters and getters have no explicit implementation. the implementation is internal
once we declare foo as an object with `foo = {}`, it gains these special setters and getters




`bar = foo.a`, `foo.a.x = 5`
the way this would work in flo diagram: `foo.a.x => foo => foo.a => bar`
this follows the propagation method
however, with our memory-location based triggers, we can skip the intermediate `foo` step, and do `foo.a.x => foo.a => bar`
this is the difference between name based and memory based binding systems
is memory based just an optimization?
first must prove that they both work, and do exactly the same thing

this is the real reason why the alias system works: (not reduction)
the first variable "names" the memory location, all other aliases just point to it



Summary
so for, we have explored 3 types of binding systems for reference based models
1. graph-traversal (variable-name based, adapted from value-based ideology)
2. event propagation (variable-name based, the way flo seems to work?)
3. memory location based (optimization?)



memory location based is the only one whose complexity doesn't change no matter where in the tree it is
`foo = bar` and `foo.x.x.x.x = bar.x.x.x.x` will both work the same in memory location based
for event propagation, the deeper down the tree, the further the propagation, the slower it is
for mirror-copy, the further up the tree, the larger the copy, the slower it is


ITS NOT REALLY MEMORY LOCATION BIND, because bindings should not change, but memory and data structures do
easiest way to think about it is like wires, in flo diagrams
if `a = b, b = c, c = d`, then its one long wire from `a` to `d`, with `d` being the data source, and `a` through `c` being wires coming out of it



the main issue is
the data source gets changed through the setter, so all bindings to the data source is triggered
how to trigger all aliases of the data source?
we can make each alias another data source (mirror-copy method)
we can just force the trigger to propagate (propagation method)
or we can redirect all bindings to the alias, to the data source (memory location method)


remember that currently, bindings are based on variable names
this works fine if all variables point to values
javascript (as well as many other languages) also introduces the idea of collections
`bar[0] = foo.x` can be thought of as shorthand for `bar.set('0', foo.get('x'))`
the issue arises when we have "collection aliasing", aka `foo = bar` or `foo = bar.x`, where `bar` and `bar.x` are both collections
	note: this is a special case of collection bindings, where the right side is made up of one variable, and that variable refers to a collection
	doesn't apply to things like `foo = reverse(bar)`, because here a new collection is being made
	we'll go more in detail on this later

collection aliasing allow things to change "invisibly"
eg `foo = bar`, `bar.set('x', 5)` does not change `bar`, so the `foo = bar` binding is not triggered



easy way to identify an alias binding without parsing the right hand side: check if righthandside.head is in trigger dictionary

```js
foo = bar;
x = foo.x*2; // trigger on bar.x
```



test.bind(foo, [bar, bar[0], bar[1], ...], args => {
	for (var i = 0; i < bar.length; i++) {
		foo[i] = bar[i]; // how to detect aliasing here
	}
});


### Dynamic Bindings

* So far I have only been concerned with creating bindings
* however, I realized that for collection bindings, there are many cases where I will have to remove/reassign bindings
* for example, consider `mapbind(foo2, foo, x => x*x)`
* with our current implementation of `mapbind` (shown earlier), if you add another element to `foo`, then there will be double bindings on all the rest of the elements!
* also, consider `reversedFoo = reverse(foo)` where `foo` and `reversedFoo` are both collections, and `reverse` simply rewires `foo`
* what happens when `foo` starts out with one item and adds another?

```
  foo[0] ---> reversedFoo[0]                foo[0] -. .-> reversedFoo[0]
                                                     X
                                            foo[1] -' '-> reversedFoo[1]
```
* the alias binding of `foo[0]` needs to be reassigned!
* lastly, consider a demultiplexer: `output = x? foo : bar`

```
  foo --
          .----> output
  bar ---'
```
* this doesn't have any data collections, but it still has dynamic bindings!
* this helped me realize that *collection bindings are not about data collections*, or bindings involving arrays/sets
* collection bindings are about defining sets of inner, dynamic bindings
* when the input to a collection binding changes, the inner bindings change
* further discussed in the section "Grouping Bindings"

### Alias Graphs

* let's revisit what we said earlier: when the input to a collection binding changes, the inner bindings change
* compare this to a normal binding: when the input value to the binding changes, the output value changes
* for a collection binding, the output value is the set of inner bindings
* if you have multiple collection bindings chained after eachother, then binding changes propagate along the chain, just like value propagation in a chain of normal bindings
* lets see how the aliasing algorithm factors into this

* construct a graph of alias bindings

```
  foo --
          .----> output
  bar ---'
```
* when a early alias binding changes, the change propagates through the graph, so that the normal bindings at the ends can be triggered to re-compute the source aliases
* if you think about it, when the alias algorithm bindings something like `[foo.x] --> { barx2 = bar.x*2 }`, we are saying "the value of bar.x is foo.x"
* so when the alias bind changes, its like saying "the value of bar.x is now fizz.x"
* so alias bindings really do work very similarly to how normal bindings work, propagate through the graph and updating "values"

### Update Order **[+ KEY POINT!!!]**

* consider something like `foo = reverse(bar)` and `bar2 = bar[1]`
* we have one dynamic collection alias binding, and one regular alias binding
* lets say `bar` starts as `[4, 2, 5]` and gets reassigned to `[3, 4]`
* we expect the end result to be `foo = [4, 3]` and `bar2 = 4`
* notice that two (sets of) updates happen in parallel here
	* the dynamic binding for `foo` gets triggered, reconstructing the inner bindings
	* the value bindings (the inner bindings of `foo`, and the binding for `bar2`) get updated with new values
		* more specifically, we the updates `bar[0] = 3`, `bar[1] = 4`, and `bar[2] = undefined`
* first notice that, with our current model, the update order doesn't matter in regards to the end result (see key point below)
* however, note that if the value bindings update before the dynamic binding, then `foo[0]` will temporarily show `undefined`
* in addition, at least for `foo`, the value bindings can be completely ignored, because the dynamic binding re-evaluates the input values anyways
* however, for `bar2` the value bindings clearly can't be ignored
* so whats happening?
* we can think of Flo/Wijit as a hierarchy of stages, one static and two dynamic
	1. At the highest level, we have the Wijit code or Flo drawing. This definition is static, never changes
	2. The code defines dynamic bindings, how the values are "wired" together. These dynamic binding modules can change and reconstruct their inner bindings
		* this is the dynamic runtime graph directly defined by the code. While the modules may change, the graph between them does not
	3. The dynamic bindings create a graph of value bindings. This stage contains all the values for the current state of the program

	*  ================================================= WHAT ABOUT ALIAS BINDINGS =============================================================

* notice how everything consists of two dynamic components: the "wiring" between the values, and the values themselves
* when `bar` updates and the dynamic binding for `foo` updates, that is the wiring changing
* when the inner bindings from `bar` update, that is the values updating
* now we see why the value updates for `foo` are redundant:
	* conceptually, wiring and values change independently of eachother
	* however, for our implementation, when wiring changes (dynamic bindings re-evaluate), the values get re-evaluated too!


key point:
* as long as we always default to re-evaluation, then update order won't matter
* if at any time, a node is "invalidated" with no other information, it should be able to re-evaluate the outputs and fully recover
* this also means that events don't need to (and should try not to) carry any information





### Grouping Bindings

* we need to group the inner bindings, so that can be cleared and reassigned easily
* we can have a "binding controller" that manages all the inner bindings
	* so when an inner binding is triggered, it first triggers the "binding controller", which in turn triggers the inner binding
* for example, if we have `mapbind(foo2, foo, x => x*x)`, and currently `foo = [3,5]`, then currently there are two inner bindings
* instead of doing `bind(foo2.0, [foo.0], x => x*x)`, `bind(foo2.1, [foo.1], x => x*x)`, we instead do `bind(null, [foo.0, foo.1], controllerfn)`
* hmm, this still won't work, because the binding controller still needs to keep track of what bindings point to it
* instead, have the collection bind keep track of all triggers that point to it
* in addition, instead of storing the binding directly inside each trigger, store it under a unique id, so that the collection bind knows which bindings belongs to it
* for the unique id, just use the destination of the collection bind!
* so if we have `mapbind(foo2, foo, x => x*x)`, and currently `foo = [3,5]`:
	* the collection bind will make two bindings with the triggers `foo.0` and `foo.1` respectively
	* each of those triggers will store `foo2: innerBindingEvalFn`
	* when `foo` pops off the `5`, then the collection bind goes to `foo.1` and removes FDSAFDSA

	*  ================================================= FINISH THIS =============================================================

* another method would be to store a remove() function in each binding
* the collection bind would just store all of its bindings, and call remove() on each of them
* giving each binding a remove() function implies that the bindings can be passed around, and anybody can call remove()
* however, this breaks encapsulation: all inner bindings belong to the collection bind, so that collection bind should be responsible for creation and destruction
* note that both of these methods require a special "bind" function for inner binds

### Modules

* I need to pass in a special alias and normal bind function to the inside of the collection bind
* for example, a "reverse" collection bind would look like:

```js
FloModule.bind(dest, [src], (innerbind, inneraliasbind, array) => {
	for (var i = 0; i < array.length; i++) {
		inneraliasbind(dest+'.'+(array.length-i), src+'.'+i);
	}
})
```
* I could group `innerbind` and `inneraliasbind` into a object:

```js
FloModule.bind(dest, [src], (innerscope, array) => {
	for (var i = 0; i < array.length; i++) {
		innerscope.aliasbind(dest+'.'+(array.length-i), src+'.'+i);
	}
})
```
* notice that FloModule already has the `bind` and `aliasbind` functions, so it seems like for `innerscope` I could just pass in a new FloModule object
* this made me realize that thats exactly what dynamic binds do
* dynamic binds create new binding modules
	* kind of like how in javascript, functions define new scopes
* for example, in the `reverse` bind above, lets assume `src` is an array of length 10
* the `reverse` bind will create a FloModule that has alias bindings that rewire 10 inputs to 10 outputs
* when `src` changes to length 11, then the `reverse` bind needs to destroy the old FloModule, and recreate one with 11 inputs and 11 outputs
* now let's consider what should make up a FloModule

### Modules (continued)

* A FloModule should be a self-contained definition of bindings/aliasbindings
* when you connect two modules together, you need to define how to map the outputs of the first, to the inputs of the second
* for example, if you had two FloModules `add(a,b) { c = a+b }` and `square(x) { x2 = x*x }`
* to connect them, we first define a new module and its inputs and outputs
* lets give it 2 inputs `m,n` and 1 output `p`
* then we map `m,n` to `a,b` of the `add` module, and map the `c` output of the add module to `x` of the `square` module, then map `x2` to `p`
* notice how these mappings prevent variable collision
	* Eg if `square` also defined its output as `c`, it wouldn't collide with `add` because the outer module maps both `c`s to different variables
* so whenever you import and use a module, you have to define the mappings between the variables in your module, and the inputs/outputs of 
* collection and dynamic bindings are special in that they should automatically map the inner module

### Garbage Collection

* if inner FloModules are like javascript scopes, then perhaps we can use javascripts method for cleaning up scopes jJFKDSLAJFD
* in javascript, when variables are defined in a scope, and then control leaves the scope, then all the variables are "marked" for garbage collection
	* "marked" as in no more pointers to it
* so in our case, when a dynamic bind needs to recreate its inner module, it could simply invalidate the old module, insteaad of removing all the old bindings
* we also need to ensure that triggers do not trigger the invalidated bindings
* to solve this, we can have a timestamp as an id for each created module, and each binding in that module also contains that id
* when a trigger leads to a inner module, it first checks the timestamp of the binding and the module, and if it doesn't match, then that binding is invalidated and gets removed
* this "lazy" removal that only occurs during a trigger should save a lot of time, in case some invalidated bindings are never triggered, they don't need to be removed





### Alias Bindings == Dynamic Bindings

* an important thing to note is that, while dynamic bindings and value bindings are a fundamental way Flo works, alias bindings is an optimization in the implementation
	* consider `foo = bar` and `x = foo.x`, and `bar.x` changes
	* most logical response: `foo` updates (including all its children), which in turn updates `x` when `foo.x` updates
	* this is the "event propagation" method discussed earlier
	* the "copy value" method and "alias binding" methods are both optimizations
* notice that alias bindings involve removing and recreating bindings
	* eg, `output = multiplexor ? inputA : inputB`, `x = output.foo`
	* when `multiplexor` changes, `x` switches between binding with `inputA` and `inputB`
* thus, any binding attached to an alias is a dynamic binding!
* another way of looking at it is, using the previous multiplexor examples, `x = aliasSource[output].foo`





notice that value bindings, dynamic bindings, and alias bindings have rather different mechanisms
what if we mix them?
`foo = multiplexor ? myValue : myArray`
the issue is that, bindings to `foo` are different based on whether `foo` is an alias or not
if `foo` is an alias, we use a dynamic bind, otherwise we just use a static bind

solution: everything is an alias bind? because everything is based off memory based binding?



* note that for the following statements, "outer" refers to the scope of the containing module, and "inner" refers to the scope of the inner module
* to make modules (aka dynamic bindings) simple, we reserve a namespace in the outer module's `bindings` dictionary specifically for inner modules
	* so no need for anything special when creating the module's inner bindings, just prepend the namespace prefix
* we identify each inner module by the outer variable mapped to its first output, which is guaranteed to be unique (because each output maps to exactly one binding)
* we only store the dynamic binding's local inner variables in the reserved namespace
	* inputs and outputs are simply replaced by the outer variables that are mapped to them
	* so if we have `InnerModule { y = x*x; z = y % 5 }` and `OuterModule { InnerModule(foo = x, bar = y); fizz = bar+5; }`, the outer bindings would look like:
	`values: { foo: 3, bar: 9, fizz: 14, __innerBindings: { bar: { z: 4 } } }`
	* notice that "local inner variables" are just outputs that are not mapped to any outer variables
	* also notice that `z` is never used, so it can be discarded. This is where "observed outputs" comes into play (discussed later on)



* 




for now we've only been considering binds with a single output
but it's possible for binds to have multiple outputs
consider a function `quotient_remainder(a,b)` which finds the quotient and remainder of `a/b`
while this could be done with two bindings, but a single bind makes more sense and is more efficient
	another, more complex example would be a left+inner+right join of two sets, where there would be three outputs:
		1. elements only in the left set
		2. elements in both sets
		3. elements only in the right set






```js
new FloModule('integer_divider', function () {
	this.bind('my_divider_module', ['dividend','divisor'], (output, dividend, divisor) => {
		output.remainder = dividend%divisor;
		output.quotient = (dividend-output.remainder)/divisor;
	});
});

// which is equivalent to

new FloModule('integer_divider', function () {
	this.bind({__namespace__: 'my_divider_module'}, ['dividend','divisor'], (output, dividend, divisor) => {
		output.remainder = dividend%divisor;
		output.quotient = (dividend-output.remainder)/divisor;
	});
});
```


```js

new FloModule('integer_divider', function () {
	this.bind(['quotient','remainder'], ['dividend','divisor'], (output, dividend, divisor) => {
		output.remainder = dividend%divisor;
		output.quotient = (dividend-output.remainder)/divisor;
	});
});

// which is equivalent to

new FloModule('integer_divider', function () {
	this.bind({quotient: null, remainder: null}, ['dividend','divisor'], (output, dividend, divisor) => {
		output.remainder = dividend%divisor;
		output.quotient = (dividend-output.remainder)/divisor;
	});
});

// which is equivalent to

new FloModule('integer_divide', function () {
	this.bind({quotient: 'quotient', remainder: 'remainder'}, ['dividend','divisor'], (output, dividend, divisor) => {
		output.remainder = dividend%divisor;
		output.quotient = (dividend-output.remainder)/divisor;
	});
});
```




* notice that timestamps are only needed for alias binds and binding references/collections
* consider what it would look like with primitive value bindings (no references/collections), but still with dynamic bindings


```
code:

   b = a*2
   d = x ? b : c
   e = d+5

value graph:

  a ---> b ---.
               '----> d --> e
         c --
```

* notice how, even though there is a dynamic bind, if we think of the dynamic bind as a switch, and visualize update-events propagating through each node from left to right, then there is no need for timestamps
* however, notice how alias bindings complicate things:

```
code:

   b = {myprop: a}
   d = x ? b : c
   e = d.myprop

value graph:

   .-----(alias child)---.
  a ---> b ---.           '.
               '----> d --> e
         c --

(note: "alias child" refers to a child binding dynamically created by alias bindings)
```
* notice how when `x` changes, the `d` will switch from `b` to `c`, but when `a` updates, `e` will still get updated
* because alias bindings create a "jump" from `a` to `e`, even if we somehow propagate updated alias information along the normal route, it will be too late
	* the "alias child" bind allows the value propagation to jump ahead of the normal route
	* by the time we can update `e` with the new alias information, it will have already been updated with `a`'s value, so there will be a flicker in time where `e` has the wrong value
* this is why we have timestamps, so that the "alias child" binding gets invalidated instantly after `x` changes
* however, note that adding timestamps means now before triggering any binding, we have the check the timestamp
* this slows down every trigger and update event
* we could restrict timestamps to alias child bindings, but that complicates things further
* remember that we considered another solution to updating alias-child bindings:
	* have whoever created the alias-child binding keep track of it, and when the alias bindings change, remove the outdated alias-child binding
* this is better, because it doesn't slow down the update-events, and only slows down alias-child-binding creation by a tiny bit (to keep track of created bindings)


### More Efficient Event Propagation (Set-Route Event Propagation)

* in the previous section, we looked at how update events propagate through a network of only primitive value bindings
* update events simply propagate through each node in the value graph
* this is the simplest and most intuitive way of thinking about how Flo/WIjit works
* conceivably, we can extend this to reference/collection bindings
* consider this graph

```
code:

   b = {myprop: a}
   d = x ? b : c
   e = d.myprop

value graph:

  a ---> b ---.
               '----> d --> e
         c --
```
* lets say `a` changes from  3 to 5
* this triggers `b`, which doesn't do anything, but passes the event to `d`, who also doesn't do anything, but continues passing it to `e`, who updates to the new value of 5
* note that `d` needs to know that the update originated from the `myprop` child of `a`, so that `d` knows to trigger `e`, which is bound to `myprop`
* thus, when the update event "escalates" from `a` to its parent `b`, the event carries the information of which child was updated, so at the end of the line `d` knows which child binding to trigger
* for this type of event propagation to work, **these events need to carry information**
* notice the distinction between this event propagation, and the event propagation discussed much earlier (in the section "Deep Property Binding Revisited")
* in the old event propagation, events are just propagated everywhere during run time
* in this new method, the routes for event propagation are set during "compile time" (when the dynamic=bindings creates the value graph)
* thus, there are no "wasted" events. An event will only propagate up to parents if a binding is explicity defined for it
* in addition, an event can "jump" multiple parents, eg if in the example above `b = {foo: {bar: a}}`, then when the event propagates from `a` to `b`, its jumping to the grandparent
* this event propagation strictly follows the value graph, so it is much more efficient, and also very intuitive

* also, set-route event propagation always follows the graph, instead of creating "jumps" like in the alias binding method
* thus, there is no need to worry about timestamps or removing "jump" bindings

### Alias Bindings vs Set-Route Event Propagtion


```
code:

   b = {myprop: a}
   d = x ? b : c
   e = d
   f = e
   g = f.myprop

value graph:

  a ---> b ---.
               '----> d --> e --> f --> g
         c --
```


* in the first





### Triggering Nested Property Bindings

* we have talked about the copy method, event-propagation method, and alias method to solve the issue of aliases
* more specifically, the issue of `foomirror = foo` and being able to trigger `foomirror.x` when `foo.x` changes
* however, there is one last issue we need to solve
* `x = a.a.a`, we need to trigger this binding whenever `a`, `a.a`, and `a.a.a` changes
* from a different perspective: if `foo = {a, b: {x, y}}`, and `b` changes, we need to trigger bindings on `b`,`x` and `y`
* in essence, this is a reverse-prefix lookup problem:
	* given a prefix, return all words in the dictionary with that prefix
	* (in our case, given a node in a tree, return all bindings that are attached to descendents of that node)
* we actually discussed this exact issue much earlier in the section "Deep Property Binding", and came up with 3 solutions:

1. funnel bind: keep as one binding, with three triggers `[a, a.a, a.a.a]`
2. incremental bind: separate into two bindings `temp = a.get('a')`, `x = temp.get('a')` with two triggers each
3. runtime propagation: during runtime, when a parent is updated, trigger all children (and all their children, etc...)

* consider a network with M nested property bindings, each nested to N layers deep
* the last solution is the simplest to analyze: takes up no space, but event propagation is O(N) at worst (updating the highest ancestor)
	* however, every object in the value graph is impacted, with events propagating through the properties of every object, even if there are no nested property bindings
	* thus, event propagation is significantly slowed down, and there is a lot of wasted propagation
* the first solution requires N triggers for every property binding, so M*N space, but event propagation is instantaneous, O(1)
* the second solution, at worst, also requires N triggers for every property binding. However, event propation is O(N) (has to go through all intermediate ancestors)
	* no wasted propagation though. Kind of similar to the set-route event propagation method
* the second solution gets more interesting when the network becomes more interconnected
* consider a tree-like network, with nested property bindings at every leaf node:

```
                         _ a _
                        /     \
                       a       b
                      / \     / \
                     a   b   c   d
                    / \ / \ / \ / \
                    a b c d e f g h
```
* here, the first method would still take up O(M*N) space
* however, the second method would only have one binding at each node, so a total of 2M-1, or O(M) space
* the second method only reaches its worse case of O(M*N) space in a completely disjoint network, where every nested property is completely separate from each other
* the more interconnected the network, the lower the space cost, and the more sparse the network, the higher the space cost
* the second solution is like a compromise between the first and third: not as much space as the first, but not as slow as the third
* it seems like the first solution is more advantageous for shallower networks (N << M)
* the third solution is better for populated networks (# of bindings >> # of nodes)
	* imagine a very popular data channel, like the weather forecast, where tons of websites and apps have bound to this data
	* every node in this value graph would have thousands of bindings, so in the third method where every update propagates to all descendants, there would be no wasted propagations
	* in this case, the first and second methods would be extremely inefficient space-wise

### Deep Bindings

* Note that so far, we have been considering nested property bindings, aka binding to nested properties of an object, eg `foo.a.a = x`
* as with any bind, this only detects when that specific reference/value changes
* for imperative code, we oftentimes want to detect when any nested property changes, not just when the top-most reference changes
* eg if we have some imperative code module that adds up all the integer-value descendants of `foo`
* if we just created a binding to to `foo`, then our module wouldn't be triggered when `foo.x` changes
* thus, we need a "deep bind", a binding that gets triggered whenever a descendant of `foo` changes
* note that if we deep bind `foo.a.a`, it will also be triggered when `foo` or `foo.a` changes, but that's just a part of nested property binding, not a part of deep binding
	* deep binding only monitors descendants

Two ways to implement deep binding:
1. prefix checking
	* check if changed node is descendant of deep bind root (the input to the deep bind)
	* aka if we deepbind `foo.a.a`, then we need to trigger on things that `foo.a.a.` is a prefix of, eg `foo.a.a.bar`
	* if we are deepbinding X and we change Y, we trigger the binding if X is a prefix of Y
2. recursive bind
	* same mechanism as the copy/mirror bind mentioned earlier
	* create a listener at the deep bind root that clones itself for each child (which will clone to grandchildren etc etc until all descendants have  a copy of that listener)
	* this method will monitor every single node under the deep-bound tree, and if any node changes, it will automatically re-configure itself to monitor any new nodes added to the tree
	* eg if we deepbind `foo.a.a`, and `foo.a.a.bar` changes, all listeners from the old descendants of `foo.a.a.bar` are removed, and new listeners are added for each of the current descendants of `foo.a.a.bar`
	* to account for circular references, we do a graph search (keep track of visited nodes when adding listeners)

Comparison:
* prefix checking requires a new check for every single "set" operation
* recursive bind is pretty slow and expensive, but only affects the deep bind
* recursive bind also doesn't introduce any new mechanisms

* **deep bindings are syntactic sugar**
* notice how deep bindings can be created from recursive dynamic bindings
* they are an added feature that makes it easier to mix and embed imperative code into Flo modules
	* eg, instead of using a complicated recursive dynamic bind to flatten a tree into an array, just do a depth-first-search with imperative code
* later, in the "pure" version of Flo, deep bindings should not be possible
* deep bindings lack the advantages that recursive dynamic binds have: tracking internal changes without affecting the whole object
	* after flattening a tree into an array, if you change one value in the tree, you only need to change one value in the array, but the imperative deep-bind version will re-run the depth-first-search and reconstruct the entire array
* thus, we should use the 2nd method (recursive binding) so deep binding is a built-in function that requires no special mechanism, and can be removed easily


================================================== CHECK UR GOOGLE KEEP NOTES ===================================================================




### Dynamic Collections Bindings are Based on Structure

* consider the dynamic `reverse` bind, which takes an array and wires an output array such that the output elements are the input elements in reverse order
* `reverseFoo = reverse(foo)`
* when do we want the dynamic bind to be triggered, aka when do we need to re-wire the value graph?
* we need to know when `foo` has been re-assigned, but that isn't enough
* we also need to know when an element has been added or removed
* however, we can't just simply listen to all properties+values of `foo`, because we don't need to know when an existing element's value changes
	* the value graph will handle that
* notice that essentially, we are listening to when the _structure_ of `foo` changes
* this makes sense, because when the structure changes, the wiring would have to change too
* thus, for javascript objects, the structure changes when
	1. the main pointer is reassigned
	2. a property is added
	3. a property is removed
* this is only for dynamic binds though. Normal binds would just check the value of the pointer
	* because in normal binds, you can't monitor structure in the first place
	* if you tried to make a normal bind monitor a structure, it wouldn't work, because you wouldn't know when child values change
	* eg if you made a normal bind that 
* this gets a little ugly for something like `output = foo == bar ? 1 : 2` (assuming foo and bar are both objects), because this binding is based only on the value of the pointers, not on the actual structure
	* wasted re-evaluations when foo/bar change structure, because the value of the pointers don't change
	* ideally, the compiler would be able to detect this and use a normal bind instead of a dynamic bind

### Nested Binding vs Dynamic BInding

* remember how much earlier (in the section "Collection Listeners Revisited"), we talked about how collection bindings create "sub-bindings" which absorb the onset event?
* lets revisit that with an example:
* lets say you have a collection binding, that binds to the array of phrases `savage[]` and:
	* counts the number of phrases with the word `rekt`, outputting it to `rektCount`
	* constantly outputs elements 1,3, and 5 to the object `onethreefive = {one: ___ , three: ___ ,five: ___ }`
* lets say for some reason, element three constantly alternates between `undefined` and `0`
* because a value of `undefined` implies the structure has changed (key-value entry was removed), then this means that the dynamic binding constantly gets updated
* however, notice that we can make things a little more efficient by separating the `onethreefive` output to separate, non-dynamic binding
* this was, when element 3 alternates, it doesn't need to recreate the entire `onethreefive` object over and over again
* we can use a normal binding because `onethreefive` is not based on structure, it is based on value
* this is the difference between a nested binding (value based) and a dynamic binding (structure based)

### Automatic Detection of Dynamic Binds

* from the example in the previous example, notice how normal bindings and dynamic bindings respond to `undefined` values a little differently
	* normal bindings (bound to nested values in this case) stay attached to the memory location, and the output is set to undefined
	* dynamically created bindings disappear when the value goes undefined, so the output doesn't exist anymore (also undefined, but for a different reason)
* non-nested normal bindings work too
* consider two ways of defining `x2`:
	* `bind(x2 = x*2)`
	* `dynamicbind { if (x !== undefined) then bind(x2 = x*2) }`
* both result in the same output
* while its normal for a language to have multiple ways of defining the same behavior, this feels like it needs more inspection
* it seems like the main difference is that when x is undefined:
	* with the normal binding, the binding is still there, but the value of `x2` is set to undefined
	* with the dynamic binding, the binding disappears, and the value of `x2` is implicitely undefined
* perhaps determining whether a binding is dynamic or normal should be implicit, not explicit
* in order to automatically detect dynamic bindings, we need to identify and categorize a few key cases
* some indisputably dynamic bindings include:
	* reverse bind: `for (n in foo) { reversed[foo.length-n-1] = foo[n] }`
	* demultiplexor: `a = x ? b : c` where `b` and `c` are collections (or perhaps it shouldn't matter if `b` and `c` are collections, because variables can change between value/collection at any time)
* it seems like **any time the memory location of a variable is dynamic, you need a dynamic bind**
* thus, in the case of `x2`, the memory location never changes (even in the dynamic bind, because without a binding, `x2` just becomes undefined), so it should be a normal bind

### Optimizations: Breakdown into Atomic Statements, Joining Shared Paths, and Reducing Paths

* statements can be broken down to atomic parts, helping to isolate dynamic and normal binds
* eg `a = x ? b : c*5` can be broken down to `c5 = c*5` (normal bind) and `a = x ? b : c5` (dynamic bind)
* breaking bindings into atomic parts allows the compiler to determine which variables can be shared and which paths can be joined
* eg `a = x ? b : c*5` and `d = a+c*5` can be broken down to `c5 = c*5`, `a = x ? b : c5`, `d = a*c5`, thus allowing `c5` to be shared
* in addition, bindings can be joined together if they are part of a "single path"
* eg, if `a = b*5`, `b = c+1`, and `c = d%3`, and there are no other bindings to `b` and `c`, then we can reduce the three statemens to `a = ((d%3)+1)*5`
* notice how a "single path" occurs when output variables are used as input to only one binding, allowing us to substitute that input with the expression for that input
	* in the most recent example, we substituted `c` for `d%3` in the second statement, and then substituted `b` for `(d%3)+1` in the first statement
* thus, the order of steps is

1. Deconstruction: Break statements down into atomic binds
2. Comprehension: Find shared variables, and join the paths
3. Reduction: substitute single-use variables with their binding expression

### Combined Funnel and Incremental Bind: 

* remember:
	* funnel bind: one binding with three triggers `[a, a.a, a.a.a]`
	* incremental bind: separate into two bindings `temp = a.get('a')`, `x = temp.get('a')` with two triggers each
* minor optimization: if one funnel bind completely "encloses" another funnel bind, then use the smaller funnel bind in the larger one
* if we have `x = a.a.a` and `y = a.a.a.a.a`, then instead of making `[a, a.a, a.a.a, a.a.a.a, a.a.a.a.a]` the triggers for `y`, just make it `[x, a.a.a.a, a.a.a.a.a]`

```
    a -.
    a --\
    a ---- x -.
    a ---------\
    a ----------- y
```

* while this means `y` is delayed by one step, this extra step is being used to trigger `x`, so there's no waste (at least in a single-threaded system)
* note that this only works for "linear structures", aka when one funnel completely encloses the other because their triggers are along the same path
* note that for if `y` only partially enclosed `x` (not fully enclosed but some shared ancestors), then no optimization is used
	* see "Tree Variant" below for a different take
* advantages:
	* for single-threaded systems, same speed as funnel bind
	* for linear structures, takes up the same space as incremental bind (same number of triggers)
* disadvantages:
	* slower setup time in some cases (if `x` is bound after `y`, then `y`'s funnel has to be restructured)
	* in a multithreaded system, this prevents `x` and `y` from being triggered at the same time
	* more complicated to remove bindings in some cases (if `x` is removed, then `y`'s funnel has to be restructured)
	* for tree structures, takes up more space than incremental bind

Tree Variant:
* if a funnel bind doesn't completely enclose another funnel bind, but shares some ancestors, than create an intermediate binding for the shared ancestors
* eg for `x = a.a.a` and `y = a.a.b.b`, then add intermediate binding `z = a.a` and use `z` in the triggers of `x` and `y`
* advantages: always uses the same amount of space as incremental
* disadvantages: every intermediate binding slows it down a little

### Nested Property Binding: Graph Representation

* what would nested property binding look like in a Flo network? how would it propagate through the value graph?
* the value graph is the most intuitive way of looking at Flo, and is what Flo is based on, so studying how it works in the graph helps understand the core issue
	* note: set-route event propagation adheres closest to the Flo graph representation
* consider `bind(y = foo.x.x`
* as we mentioned in the section "Deep Property Binding", this can be thought of as `bind(y = foo.get('x').get('x')`
* instead of having a funnel bind with 3 triggers, we have three atomic bindings (including the one that creates `foo`): `foo = ...`, `temp = foo.x`, `y = temp.x`
* any one of `foo`, `foo.x`, and `foo.x.x` changing will correspond to an update of exactly one of those atomic bindings
* the funnel bind is just a reduction optimization that compresses those three atomic binds into one bind with 3 triggers
* if we decompose all nested property bindings into atomic bindings, and follow this reduction optimization, the graph turns into the tree-optimization-variant of funnel bind

### Reference Binds = Alias Binds

* the name "alias binding" makes it seem like they are a special case of a object binding, where the left side exactly equals the right side
* doesn't make it obvious that alias binds and normal binds are all that are needed to create a complete binding model for a reference based data system
* this is because alias bindings are really "reference" bindings, analogous to pointers or references
* thus, any data structure can be decomposed into values and references

* value bindings are for defining values
* reference bindings are for defining structures

### Main Interface

* in order to actually complete a working implementation of Flo/Wijit without worrying about this optimizations, we have to create a high level interface that we can roughly implement now, and optimize later, without too much refactoring
* thus, we need to summarize all findings so far:

Bindings:
	1. Value bindings: creates a value
	2. Reference bindings: creates a reference
	3. Dynamic bindings: creates more bindings
Triggers:
	1. Normal triggers
	2. Nested binding triggers
Modules: multiple dynamic bindings
Built-ins:
	* Map bind
	* Reduce bind
	* Filter bind
	* Deep bind

* this is re-iterated in the section: Summary>Mechanisms

### Atomic Bindings

* ideally, we only create one binding at a time (corresponding to one output at a time)
* this should be possible because the value graph is a DAG, so we'll just be building the DAG one node at a time in the "forward" direction
* however, this gets a little complicated with dynamic bindings, because currently for list bindings (eg map bind), we use a for-loop and some imperative code to create all the output bindings at once
* the solution to this is covered in the next section

### List Iteration with Dynamic Bindings

* one way to get rid of for-loops is to use linked-list-style array iteration, like how most functional and logic languages work (eg lisp, prolog)
* this way instead of accessing elements by index, we do as follows:
	1. create a binding for the current element
	2. create a dynamic binding to check for the existence of a next element
		* and if the element exists, execute these steps for the next element
* thus, instead of a giant dynamic binding that uses for-loops (aka imperative code), we use a chain of dynamic bindings
* in addition, this means we only create one binding at a time, achieving atomic dynamic bindings (at least for list iteration)
* we also aren't introducing any new mechanics, just leveraging object properties
* iterating across object properties will follow the same convention
	* maintain a linked list of all registered properties for each object
* will automatically detect when a property is added or removed
	* takes care of "structure listening" as discussed in the section "Dynamic Collections Bindings are Based on Structure", without introducing any new mechanisms

----------------------------------------------------------------------------------------------------

Summary
--------

All findings so far...

### Language Rules

1. Outputs (at least, when observed) always reflect the current state of the inputs
	* no matter how the language is implemented, the observed outputs should always look as if the entire network is continuously being re-evaluated
2. Bindings, by default, execute re-evaluations. This means update order doesn't matter, and update events don't need to carry any information
3. Every output is determined by exactly one binding
4. Dynamic binding definitions are static, and create/define value bindings
5. Modules are self contained. A module can have any internal implementation as long as the inputs/outputs follow these rules
6. All inputs and outputs are key/value pairs (just like in javascript), and values can point to modules (aka values can be collections)
7. Output values with no input binding are implicitly undefined
8. Undefined input values forces the outputs to be undefined (skipping evaluation of the binding function) unless the "undefined" case is explicitely accounted for in the binding function
	* this means an undefined input that is not accounted for basically "voids" the binding, making it behave as if the binding disappeared (forcing the outputs undefined by rule #6)
9. Value bindings are for defining values
10. Reference bindings are for defining structures
11. Dynamic bindings are for creating more bindings

### Implementation Rules

1. Reference based, and circular references are allowed
2. Bindings are based on memory location
3. Iteration is done using dynamic binds, in a linked-list fashion (see "List Iteration with Dynamic Bindings")

### Implementation Assumptions

1. Value updates happen more than dynamic bind updates
2. sparse vs interconnected?
3. densely populated (many bindings per object property)?

### Mechanisms

* Bindings
	1. Value bindings: creates a value
	2. Reference bindings: creates a reference
	3. Dynamic bindings: creates a binding
* Triggers
	1. Normal triggers
	2. Nested-property triggers
* Modules: multiple dynamic bindings
* Built-ins
	* Map bind
	* Reduce bind
	* Filter bind
	* Deep bind

### Mechanism Implementation

Reference Binding:
* Copy
* Event propagation
* Set-Route Event Propagation
* Alias index

Nested Property Triggers:
* Funnel
	* variant: path-incremental
	* variant: tree-incremental
* Incremental
* Event propagation
* Copy

### Copy Bind

* when binding to an object, use a recursive bind to create bindings between every descendant of the input and the corresponding descendant of the output
* to account for circular references, keep track of visited nodes during the recursive bind

something like

```js
function mirror(source, dest, visited) {
	if (visited.contains(source))
		return;
	visited.add(source);
	if (source instanceof Object) {
		for (var key in source) {
			mirror(source[key], dest[key], visited);
		}
	} else {
		bind(source, dest);
	}
}
```

* because it creates a listener at every single descendant of every object that has bindings, this also (almost) takes care of nested property triggers without needing a separate mechanism
	* doesn't account for dangling bindings, aka if `y = bar.x.x`, and we set `bar = 5`, `y` doesn't get updated to undefined
* however, extremely slow and expensive

### Other Optimizations

Decomposition and Analyzing Shared Paths
1. Deconstruction: Break statements down into atomic binds
2. Comprehension: Find shared variables, and join the paths
3. Reduction: substitute single-use variables with their binding expression
